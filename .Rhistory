setwd("Z:/Projects/DESTATIS/PredErrorComplex/PPerfEstComplex")
library("mlr3")
library("mlr3learners")
library("mlr3verse")
mlr_learners$get("regr.lm")
library("dplyr")
# regression settings:
b0 <- 5
b1 <- 1
b2 <- 1
experiments <- expand.grid(list(N = c(10000, 50000, 100000),
correct_model = c(TRUE, FALSE)
))
# Source the functions that are used in performing the calculations
# on the cluster:
source("./simulations/nsrs/functions.R")
nsim <- 10
lgr::get_logger("mlr3")$set_threshold("warn")
i <- 1
# Repeat:
# Generate Population + (large) test set
outlist <- list()
set.seed(1987)
starti <- Sys.time()
for(k in 1:nrow(experiments)){
N <- experiments$N[k]
n <- N/100
correct_model <- experiments$correct_model[k]
for(iter in 1:nsim){
train_data <- generate_data(N, b0, b1, b2)
test_data <- generate_data(N, b0, b1, b2)
# Calculate the auxiliary variable for PPS sampling
valid <- FALSE
while(!valid){
u <- train_data$y + rnorm(N)
if(all(u>0)){
valid <- TRUE
}
}
# Draw PPS-sample
w <- n*u/(sum(u))
ids <- sample(1:N, n, prob = w)
sample_weights <- w[ids]
# if not the correct model exclude one of the covariates
if(!correct_model){
train_data <- train_data %>% select(-x2)
test_data <- test_data %>% select(-x2)
}
# Take the sample
train_data <- train_data[ids,]
# Estimate generalization performance via CV (mlr3)
task_sp <- as_task_regr(y ~ ., data = train_data)
measure <- msr("regr.mse")
# Define the experiment setup
learnerlist <- c(lrn("regr.lm"),
lrn("regr.ranger")
)
design <- benchmark_grid(
tasks = task_sp,
learners = learnerlist,
resamplings = rsmps("cv", folds = 5)
)
bmr <- benchmark(design)
tab <- bmr$aggregate(measure)
estimated_error <- tab$regr.mse
corrected_estimates <- HT_correction(bmr, w, N)
# Use test set to get "true" generalization performance
true_error <- c()
for(j in 1:length(learnerlist)){
learnerlist[[j]]$train(task_sp)
outpreds <- learnerlist[[j]]$predict_newdata(test_data)
true_error[j] <- outpreds$score(measure)
}
outlist[[i]] <- data.frame(iter = iter,
n = n,
correct = correct_model,
model = tab$learner_id,
estimated_error = estimated_error,
corrected = corrected_estimates,
true_error = true_error
)
cat(paste0("Iteration: ", i, " of ", nrow(experiments)*nsim), "\n")
i = i+1
}
}
library(future)
# Detect the number of available cores
plan(multiprocess)
?availableCores
parallelly::availableCores()
rm(list=ls());gc()
# Set working directory:
#setwd("~/PPerfEstComplex")
setwd("Z:/Projects/DESTATIS/PredErrorComplex/PPerfEstComplex")
library("mlr3")
library("mlr3learners")
library("mlr3verse")
mlr_learners$get("regr.lm")
library("dplyr")
# regression settings:
b0 <- 5
b1 <- 1
b2 <- 1
experiments <- expand.grid(list(N = c(10000, 50000, 100000),
correct_model = c(TRUE, FALSE)
))
# Source the functions that are used in performing the calculations
# on the cluster:
source("./simulations/nsrs/functions.R")
nsim <- 10
lgr::get_logger("mlr3")$set_threshold("warn")
i <- 1
# Repeat:
# Generate Population + (large) test set
outlist <- list()
set.seed(1987)
starti <- Sys.time()
for(k in 1:nrow(experiments)){
N <- experiments$N[k]
n <- N/100
correct_model <- experiments$correct_model[k]
for(iter in 1:nsim){
train_data <- generate_data(N, b0, b1, b2)
test_data <- generate_data(N, b0, b1, b2)
# Calculate the auxiliary variable for PPS sampling
valid <- FALSE
while(!valid){
u <- train_data$y + rnorm(N)
if(all(u>0)){
valid <- TRUE
}
}
# Draw PPS-sample
w <- n*u/(sum(u))
ids <- sample(1:N, n, prob = w)
sample_weights <- w[ids]
# if not the correct model exclude one of the covariates
if(!correct_model){
train_data <- train_data %>% select(-x2)
test_data <- test_data %>% select(-x2)
}
# Take the sample
train_data <- train_data[ids,]
# Estimate generalization performance via CV (mlr3)
task_sp <- as_task_regr(y ~ ., data = train_data)
measure <- msr("regr.mse")
# Define the experiment setup
learnerlist <- c(lrn("regr.lm"),
lrn("regr.ranger", num.threads=parallelly::availableCores())
)
design <- benchmark_grid(
tasks = task_sp,
learners = learnerlist,
resamplings = rsmps("cv", folds = 5)
)
bmr <- benchmark(design)
tab <- bmr$aggregate(measure)
estimated_error <- tab$regr.mse
corrected_estimates <- HT_correction(bmr, w, N)
# Use test set to get "true" generalization performance
true_error <- c()
for(j in 1:length(learnerlist)){
learnerlist[[j]]$train(task_sp)
outpreds <- learnerlist[[j]]$predict_newdata(test_data)
true_error[j] <- outpreds$score(measure)
}
outlist[[i]] <- data.frame(iter = iter,
n = n,
correct = correct_model,
model = tab$learner_id,
estimated_error = estimated_error,
corrected = corrected_estimates,
true_error = true_error
)
cat(paste0("Iteration: ", i, " of ", nrow(experiments)*nsim), "\n")
i = i+1
}
}
results <- do.call('rbind', outlist)
# Visualise the results and analyse the bias:
# compare unbiased and biased version of estimates
ggframe <- rbind(results %>% mutate(type = "biased"),
results %>% mutate(estimated_error = corrected,
type = "de-biased (HT)")
) %>%
mutate(bias       = estimated_error - true_error)
results %>% group_by(model, n, correct) %>% summarise(bias = mean(corrected - true_error))
results %>% group_by(model, n, correct) %>% summarise(bias = mean(estimated_error - true_error))
ggframe$type
ggframe$correct
ggframe$correctmod <- "model correctly specified"
ggframe$correctmod[!ggframe$correct] <- "model misspecified"
ggframe$correctmod <- factor(ggframe$correctmod, levels=c("model correctly specified", "model misspecified"))
unique(ggframe$n)
ggframe$n <- paste0("n = ", ggframe$n)
ggframe$n <- factor(ggframe$n, levels=c("n = 100", "n = 500", "n = 1000"))
ggframe$model[ggframe$model=="regr.lm"] <- "linear models"
ggframe$model[ggframe$model=="regr.ranger"] <- "random forests"
ggframe$model <- factor(ggframe$model, levels=c("linear models", "random forests"))
# look at all settings jointly
p <- ggplot(data=ggframe, aes(y=bias, x = model, fill = type)) +
geom_boxplot() +
facet_wrap(~correctmod+n, nrow=2, scales="free_y") +
theme_bw() + theme(axis.title.x=element_blank(),
axis.title.y=element_text(size=14),
axis.text.x = element_text(angle=45, hjust = 1, color="black", size=12),
axis.text.y = element_text(color="black", size=11),
strip.text = element_text(size=12),
legend.position = c(0.75, 0.9))
library("ggplot2")
# look at all settings jointly
p <- ggplot(data=ggframe, aes(y=bias, x = model, fill = type)) +
geom_boxplot() +
facet_wrap(~correctmod+n, nrow=2, scales="free_y") +
theme_bw() + theme(axis.title.x=element_blank(),
axis.title.y=element_text(size=14),
axis.text.x = element_text(angle=45, hjust = 1, color="black", size=12),
axis.text.y = element_text(color="black", size=11),
strip.text = element_text(size=12),
legend.position = c(0.75, 0.9))
p
experiments
# Make table of settings:
scenariogrid <- expand.grid(repetition=1:100, correct_model = c(TRUE, FALSE), N = c(10000, 50000, 100000), stringsAsFactors = TRUE)
scenariogrid <- scenariogrid[,ncol(scenariogrid):1, drop=FALSE]
set.seed(1234)
seeds <- sample(1000:10000000, size=nrow(scenariogrid))
scenariogrid$seed <- seeds
set.seed(1234)
reorderind <- sample(1:nrow(scenariogrid))
scenariogrid <- scenariogrid[reorderind,,drop=FALSE]
rownames(scenariogrid) <- NULL
dim(scenariogrid)
25*600
25*600/100
25*600/100/60
30*600/100/60
rm(list=ls());gc()
getwd()
# Make table of settings:
scenariogrid <- expand.grid(repetition=1:100, correct_model = c(TRUE, FALSE), N = c(10000, 50000, 100000), stringsAsFactors = TRUE)
scenariogrid <- scenariogrid[,ncol(scenariogrid):1, drop=FALSE]
set.seed(1234)
seeds <- sample(1000:10000000, size=nrow(scenariogrid))
scenariogrid$seed <- seeds
set.seed(1234)
reorderind <- sample(1:nrow(scenariogrid))
scenariogrid <- scenariogrid[reorderind,,drop=FALSE]
rownames(scenariogrid) <- NULL
save(scenariogrid, file="./simulations/nsrs/results/intermediate_results/scenariogrid.Rda")
source("./simulations/nsrs/functions.R")
head(scenariogrid)
RNGkind()
sessionInfo()
RNGkind("Mersenne-Twister", "Inversion", "Rounding")
# Make table of settings:
scenariogrid <- expand.grid(repetition=1:100, correct_model = c(TRUE, FALSE), N = c(10000, 50000, 100000), stringsAsFactors = TRUE)
scenariogrid <- scenariogrid[,ncol(scenariogrid):1, drop=FALSE]
set.seed(1234)
seeds <- sample(1000:10000000, size=nrow(scenariogrid))
scenariogrid$seed <- seeds
set.seed(1234)
reorderind <- sample(1:nrow(scenariogrid))
scenariogrid <- scenariogrid[reorderind,,drop=FALSE]
rownames(scenariogrid) <- NULL
# Save scenariogrid, needed in evaluation of the results:
save(scenariogrid, file="./simulations/nsrs/results/intermediate_results/scenariogrid.Rda")
# Source the functions that are used in performing the calculations
# on the cluster:
source("./simulations/nsrs/functions.R")
head(scenariogrid)
evaluatesetting(1)
load("./Simulations/nsrs/results/intermediate_results/results_safe.Rda")
setwd("Z:/Projects/DESTATIS/PredErrorComplex/PPerfEstComplex")
# Load the results:
load("./Simulations/nsrs/results/intermediate_results/results_safe.Rda")
library("ggplot2")
library("dplyr")
# Visualise the results and analyse the bias:
# compare unbiased and biased version of estimates
ggframe <- rbind(results %>% mutate(type = "biased"),
results %>% mutate(estimated_error = corrected,
type = "de-biased (HT)")
) %>%
mutate(bias       = estimated_error - true_error)
results %>% group_by(model, n, correct) %>% summarise(bias = mean(corrected - true_error))
results %>% group_by(model, n, correct) %>% summarise(bias = mean(estimated_error - true_error))
ggframe$type
ggframe$correct
ggframe$correctmod <- "model correctly specified"
ggframe$correctmod[!ggframe$correct] <- "model misspecified"
ggframe$correctmod <- factor(ggframe$correctmod, levels=c("model correctly specified", "model misspecified"))
unique(ggframe$n)
ggframe$n <- paste0("n = ", ggframe$n)
ggframe$n <- factor(ggframe$n, levels=c("n = 100", "n = 500", "n = 1000"))
ggframe$model[ggframe$model=="regr.lm"] <- "linear models"
ggframe$model[ggframe$model=="regr.ranger"] <- "random forests"
ggframe$model <- factor(ggframe$model, levels=c("linear models", "random forests"))
# look at all settings jointly
p <- ggplot(data=ggframe, aes(y=bias, x = model, fill = type)) +
geom_boxplot() +
facet_wrap(~correctmod+n, nrow=2, scales="free_y") +
theme_bw() + theme(axis.title.x=element_blank(),
axis.title.y=element_text(size=14),
axis.text.x = element_text(angle=45, hjust = 1, color="black", size=12),
axis.text.y = element_text(color="black", size=11),
strip.text = element_text(size=12),
legend.position = c(0.75, 0.9))
p
# look at all settings jointly
p <- ggplot(data=ggframe, aes(y=bias, x = model, fill = type)) +
geom_boxplot() +
facet_wrap(~correctmod+n, nrow=2, scales="free_y") +
theme_bw() + theme(axis.title.x=element_blank(),
axis.title.y=element_text(size=14),
axis.text.x = element_text(angle=45, hjust = 1, color="black", size=12),
axis.text.y = element_text(color="black", size=11),
legend.title = element_text(size=14),
legend.text = element_text(size=12),
strip.text = element_text(size=12),
legend.position = c(0.75, 0.9))
p
rm(list=ls());gc()
# Make table of settings:
scenariogrid <- expand.grid(repetition=1:100, correct_model = c(TRUE, FALSE), N = c(10000, 50000, 100000), stringsAsFactors = TRUE)
scenariogrid <- scenariogrid[,ncol(scenariogrid):1, drop=FALSE]
set.seed(1234)
seeds <- sample(1000:10000000, size=nrow(scenariogrid))
scenariogrid$seed <- seeds
set.seed(1234)
reorderind <- sample(1:nrow(scenariogrid))
scenariogrid <- scenariogrid[reorderind,,drop=FALSE]
rownames(scenariogrid) <- NULL
# Save scenariogrid, needed in evaluation of the results:
save(scenariogrid, file="./simulations/nsrs/results/intermediate_results/scenariogrid.Rda")
# Source the functions that are used in performing the calculations
# on the cluster:
source("./simulations/nsrs/functions.R")
head(scenariogrid,10)
source("./simulations/nsrs/functions.R")
debugSource("Z:/Projects/DESTATIS/PredErrorComplex/PPerfEstComplex/Simulations/nsrs/functions.R", echo=TRUE)
evaluatesetting(1)
nfolds <- length(bmr$resample_results$resample_result[[1]]$predictions())
nfolds
nmods <- length(bmr$resample_results$resample_result)
nmods
error <- matrix(0, nrow = nfolds, ncol = nmods)
error
kk<-1
pred_frame <- bmr$resample_results$resample_result[[kk]]$predictions()
pred_frame
fold<-1
fold_frame <- pred_frame[[fold]]
fold_frame
true_val <- fold_frame$response
pred_val <- fold_frame$truth
true_val
pred_val
w_fold <- sample_weights[fold_frame$row_ids]
w_fold
fold_frame$row_ids
(sum((w_fold^-1)*((true_val-pred_val)^2)))/(N*(1/nfolds))
fold
nfolds
w_fold
N
sum(w_fold)
source("./simulations/nsrs/functions.R")
debugSource("Z:/Projects/DESTATIS/PredErrorComplex/PPerfEstComplex/Simulations/nsrs/functions.R", echo=TRUE)
evaluatesetting(1)
nfolds <- length(bmr$resample_results$resample_result[[1]]$predictions())
nmods <- length(bmr$resample_results$resample_result)
error <- matrix(0, nrow = nfolds, ncol = nmods)
nfolds
source("./simulations/nsrs/functions.R")
source("./simulations/nsrs/functions.R")
source("./simulations/nsrs/functions.R")
evaluatesetting(1)
source("./simulations/nsrs/functions.R")
evaluatesetting(1)
# This function performs one repetition of the five times repeated
# 5-fold stratified cross-validation on a specific data set using
# one of the fives compared methods.
#
# It takes the whole number 'iter', which corresponds to the iter-th line
# of 'scenariogrid', which contains the necessary information
# on the iter-th setting.
evaluatesetting <- function(iter) {
# Obtain information for the iter-th setting:
N <- scenariogrid$N[iter]
correct_model <- scenariogrid$correct_model[iter]
repetition <- scenariogrid$repetition[iter]
seed <- scenariogrid$seed[iter]
# Set seed:
set.seed(seed)
result <- simulation(N=N, correct_model=correct_model, repetition=repetition)
# Save results:
return(result)
}
# Function for performing the simulation for a specific setting.
# Function input:
# niter: number of simulation iterations
# N: number of clusters
# ni: number of observations per cluster
# beta: coefficients of the variables
# sdbinter: standard deviation of the random intercepts
# sdbslope: standard deviation of the random slope of variable x3
# sdeps: standard deviation of the Gaussian noise
# type: type of variables. Can be "norm" for normally distributed variables
# and "bin" for binary variables (equal probability for each class)
# fixed:
# Function output:
# No output. The MSE values resulting when dividing the data into training and test
# data (a) at the level of the observations and (b) at the level of the clusters.
# A data.frame containing the simulated data.
simulation <- function(N, correct_model, repetition, b0 = 5, b1 = 1, b2 = 1)
{
library("mlr3")
library("mlr3learners")
library("mlr3verse")
mlr_learners$get("regr.lm")
library("dplyr")
lgr::get_logger("mlr3")$set_threshold("warn")
# Repeat:
# Generate Population + (large) test set
#####N <- experiments$N[k]
n <- N/100
#####correct_model <- experiments$correct_model[k]
train_data <- generate_data(N, b0, b1, b2)
test_data <- generate_data(N=200, b0, b1, b2)
# Calculate the auxiliary variable for PPS sampling
valid <- FALSE
while(!valid){
u <- train_data$y + rnorm(N)
if(all(u>0)){
valid <- TRUE
}
}
# Draw PPS-sample
w <- n*u/(sum(u))
ids <- sample(1:N, n, prob = w)
sample_weights <- w[ids]
# if not the correct model exclude one of the covariates
if(!correct_model){
train_data <- train_data %>% select(-x2)
test_data <- test_data %>% select(-x2)
}
# Take the sample
train_data <- train_data[ids,]
# Estimate generalization performance via CV (mlr3)
task_sp <- as_task_regr(y ~ ., data = train_data)
measure <- msr("regr.mse")
# Define the experiment setup
learnerlist <- c(lrn("regr.lm"),
lrn("regr.ranger"))
design <- benchmark_grid(
tasks = task_sp,
learners = learnerlist,
resamplings = rsmps("cv", folds = 5)
)
bmr <- benchmark(design)
tab <- bmr$aggregate(measure)
estimated_error <- tab$regr.mse
corrected_estimates <- HT_correction(bmr, w, N, sample_weights)
# Use test set to get "true" generalization performance
true_error <- c()
for(j in 1:length(learnerlist)){
learnerlist[[j]]$train(task_sp)
outpreds <- learnerlist[[j]]$predict_newdata(test_data)
true_error[j] <- outpreds$score(measure)
}
result <- data.frame(iter = repetition,
n = n,
correct = correct_model,
model = tab$learner_id,
estimated_error = estimated_error,
corrected = corrected_estimates,
true_error = true_error)
return(result)
}
# Simulation settings
# regression model
generate_data <- function(N, b0, b1, b2, sigma = 1){
x1 <- rgamma(N, 0.1, 0.1)
x2 <- rgamma(N, 0.1, 0.1)
x3 <- rnorm(N)
x4 <- rnorm(N)
x5 <- rnorm(N)
y <- b0 + b1*x1 + b2*x2 + rnorm(length(x1), 0, sigma)
return(data.frame(y, x1, x2, x3, x4, x5))
}
# Horvitz Thompson correction (equation 14 from the paper)
HT_correction <- function(bmr, w, N, sample_weights){
nfolds <- length(bmr$resample_results$resample_result[[1]]$predictions())
nmods <- length(bmr$resample_results$resample_result)
error <- matrix(0,
nrow <- nfolds,
ncol <- nmods
)
for(kk in 1:nmods){
pred_frame <- bmr$resample_results$resample_result[[kk]]$predictions()
for(fold in 1:nfolds){
fold_frame <- pred_frame[[fold]]
true_val <- fold_frame$response
pred_val <- fold_frame$truth
w_fold <- sample_weights[fold_frame$row_ids]
error[fold, kk] <- (sum((w_fold^-1)*((true_val-pred_val)^2)))/(N*(1/nfolds))
}
}
apply(error, 2, mean)
}
evaluatesetting(1)
