boxplot(nonrankdataimp[nonrankdataimp$featuremethod=="rfe",]$accuracy_rf, datawideimp$accuracy_rf.rfe)
save(nonrankdataimp, file="Z:/nonrankdataimp.RData")
rm(list=ls());gc()
load("Z:/nonrankdata.RData")
nonrankdatafull <- expand.grid(selectseparately=sort(unique(nonrankdata$selectseparately)),
alwaysincludclin=sort(unique(nonrankdata$selectseparately)),
featuremethod=sort(unique(nonrankdata$featuremethod)),
cvfoldind=sort(unique(nonrankdata$cvfoldind)),
cvind=sort(unique(nonrankdata$cvind)),
dat=sort(unique(nonrankdata$dat)), stringsAsFactors = FALSE)
nonrankdatafull <- nonrankdatafull[,ncol(nonrankdatafull):1]
temp1 <- apply(nonrankdatafull, 1, paste, collapse="_")
temp2 <- apply(nonrankdata[,2:7], 1, paste, collapse="_")
fillupind <- sapply(temp2, function(x) which(temp1==x))
nonrankdatafull$auc_svm <- nonrankdatafull$brier_svm <- nonrankdatafull$accuracy_svm <-
nonrankdatafull$auc_rf <- nonrankdatafull$brier_rf <- nonrankdatafull$accuracy_rf <- NA
nonrankdatafull$auc_svm[fillupind] <- nonrankdata$auc_svm
nonrankdatafull$brier_svm[fillupind] <- nonrankdata$brier_svm
nonrankdatafull$accuracy_svm[fillupind] <- nonrankdata$accuracy_svm
nonrankdatafull$auc_rf[fillupind] <- nonrankdata$auc_rf
nonrankdatafull$brier_rf[fillupind] <- nonrankdata$brier_rf
nonrankdatafull$accuracy_rf[fillupind] <- nonrankdata$accuracy_rf
# Impute missing data:
datawide <- reshape(data=nonrankdatafull,idvar=c("dat", "cvind", "cvfoldind", "alwaysincludclin", "selectseparately"),
v.names = c("accuracy_rf", "brier_rf", "auc_rf", "accuracy_svm", "brier_svm", "auc_svm"),
timevar = "featuremethod",
direction="wide")
set.seed(1234)
library("missForest")
datawideimp <- datawide
datawideimp[datawide$alwaysincludclin==0 & datawide$selectseparately==0,-(1:5)] <- missForest(datawide[datawide$alwaysincludclin==0 & datawide$selectseparately==0,-(1:5)], ntree=200)$ximp
datawideimp[datawide$alwaysincludclin==0 & datawide$selectseparately==1,-(1:5)] <- missForest(datawide[datawide$alwaysincludclin==0 & datawide$selectseparately==1,-(1:5)], ntree=200)$ximp
datawideimp[datawide$alwaysincludclin==1 & datawide$selectseparately==0,-(1:5)] <- missForest(datawide[datawide$alwaysincludclin==1 & datawide$selectseparately==0,-(1:5)], ntree=200)$ximp
datawideimp[datawide$alwaysincludclin==1 & datawide$selectseparately==1,-(1:5)] <- missForest(datawide[datawide$alwaysincludclin==1 & datawide$selectseparately==1,-(1:5)], ntree=200)$ximp
nonrankdataimp <- reshape(datawideimp, varying=list(c("accuracy_rf.ga", "accuracy_rf.lasso", "accuracy_rf.rfe"),
c("brier_rf.ga", "brier_rf.lasso", "brier_rf.rfe"),
c("auc_rf.ga", "auc_rf.lasso", "auc_rf.rfe"),
c("accuracy_svm.ga", "accuracy_svm.lasso", "accuracy_svm.rfe"),
c("brier_svm.ga", "brier_svm.lasso", "brier_svm.rfe"),
c("auc_svm.ga", "auc_svm.lasso", "auc_svm.rfe")),
v.names=c("accuracy_rf", "brier_rf", "auc_rf", "accuracy_svm", "brier_svm", "auc_svm"),
timevar="featuremethod", times=c("ga", "lasso", "rfe"),
direction="long")
save(nonrankdataimp, file="Z:/nonrankdataimp.RData")
fix(nonrankdataimp)
names(datawideimp)
boxplot(datawideimp$auc_rf.ga)
boxplot(datawideimp$auc_rf.ga[datawideimp$alwaysincludclin==1 & datawideimp$selectseparately==1])
boxplot(datawideimp$auc_rf.ga[datawideimp$alwaysincludclin==1 & datawideimp$selectseparately==1],
nonrankdataimp$auc_rf[nonrankdataimp$featuremethod=="ga" & nonrankdataimp$alwaysincludclin==1 & nonrankdataimp$selectseparately==1])
boxplot(datawideimp$auc_rf.lasso[datawideimp$alwaysincludclin==1 & datawideimp$selectseparately==1],
nonrankdataimp$auc_rf[nonrankdataimp$featuremethod=="lasso" & nonrankdataimp$alwaysincludclin==1 & nonrankdataimp$selectseparately==1])
rm(list=ls());gc()
load("Z:/p.ad_nonrankmethod.RData")
ls()
p.ad_nonrankmethod
load("Z:/p_nonrankmethod.RData")
p_nonrankmethod
library(FSelector)
?relief
load("Z:/Projects/SideProjects/Yingxia/VariableSelectionMultiOmicsData/SecondRevision/asdf/p.ad_nonrankmethod_2.RData")
ls()
p.ad_nonrankmethod
load("Z:/Projects/SideProjects/Yingxia/VariableSelectionMultiOmicsData/SecondRevision/asdf/change_p.ad_nonrankmethod_2.RData")
ls()
p.ad_nonrankmethod
citation()
9*3+2*6+4
ls()
load("Z:/nonrankdataimp_2.Rda")
load("Z:/nonrankdataimp_2.RData")
ls()
dim(nonrankdataimp)
2*2*15
270/6
2*2*15*3
2*2*15*3*3*5
4*2*2
403.25+18.40
403.20+18.40
load("Z:/Projects/SideProjects/BlockwiseMissing/PaperCode/MitFoldwiseRF/BWM-Article/Docs/scenariogrid_foldwiseRF.Rda")
ls()
dim(scenariogrid)
remaininds <- c(8, 98, 160, 169, 188, 220, 250, 259, 278, 280, 298, 310,
340, 349, 358, 368, 386, 388, 400, 401, 430, 439, 458, 461,
464, 476, 478, 480, 486, 490, 515, 520, 524, 529, 534, 548,
551, 554, 562, 566, 568, 580, 596, 610, 613, 614, 619, 624,
638, 641, 642, 650, 652, 656, 658, 660, 661, 664, 670, 671,
686, 700, 702, 709, 714, 718, 728, 731, 733, 737, 742, 746,
748, 754, 756, 760, 776, 790, 799, 803, 804, 806, 814, 817,
818, 821, 831, 832, 833, 836, 838, 842, 844, 850, 866, 878,
880, 883, 885, 886, 889, 893, 894, 904, 908, 911, 922, 926,
928, 931, 934, 940, 956, 968, 970, 972, 973, 976, 979, 983,
984, 991, 994, 998, 1001, 1012, 1016, 1018, 1021, 1024, 1030,
1038, 1041, 1046, 1058, 1060, 1061, 1062, 1063, 1066, 1069,
1070, 1073, 1074, 1084, 1088, 1091, 1102, 1106, 1108, 1111,
1114, 1119, 1120, 1128, 1136, 1148, 1149, 1150, 1151, 1152,
1153, 1156, 1159, 1162, 1163, 1164, 1174, 1178, 1181, 1192,
1196, 1198, 1201, 1204, 1209, 1210, 1218, 1226, 1238, 1240,
1241, 1242, 1243, 1246, 1249, 1252, 1253, 1254, 1261, 1264,
1268, 1271, 1282, 1286, 1288, 1291, 1294, 1299, 1300)
remaininds2 <- c(8, 98, 160, 168, 169, 188, 220, 250, 258, 259, 274, 278, 282,
298, 310, 340, 348, 349, 358, 368, 372, 386, 388, 400, 430,
438, 439, 458, 476, 478, 490, 520, 528, 529, 548, 550, 562,
566, 568, 580, 596, 610, 618, 619, 624, 633, 638, 640, 652,
656, 658, 670, 686, 700, 704, 708, 709, 714, 723, 728, 730,
742, 746, 748, 760, 776, 790, 798, 799, 803, 804, 813, 814,
818, 820, 821, 832, 836, 838, 839, 850, 852, 866, 880, 888,
889, 893, 894, 903, 904, 908, 910, 911, 916, 922, 926, 928,
931, 940, 946, 956, 970, 972, 978, 979, 983, 984, 993, 994,
998, 1000, 1001, 1012, 1016, 1018, 1021, 1030, 1046, 1060,
1061, 1062, 1068, 1069, 1073, 1074, 1083, 1084, 1088, 1090,
1091, 1102, 1106, 1108, 1111, 1119, 1120, 1136, 1150, 1151,
1152, 1158, 1159, 1163, 1164, 1173, 1174, 1178, 1180, 1181,
1192, 1196, 1198, 1201, 1209, 1210, 1226, 1232, 1240, 1241,
1242, 1248, 1249, 1253, 1254, 1261, 1263, 1264, 1268, 1270,
1271, 1282, 1286, 1288, 1291, 1299, 130)
length(remainds2)
length(remaininds2)
length(remaininds)
library(measures)
PPV
TP
set.seed(1234)
xtr <- sample(1:4, size=100, replace=TRUE)
xest <- sample(1:4, size=100, replace=TRUE)
TP1 <- ifelse(xtr==xest & xtr==1, 1, 0)
TP1
set.seed(1234)
xtr <- sample(1:3, size=100, replace=TRUE)
xest <- sample(1:3, size=100, replace=TRUE)
TP1 <- ifelse(xtr==xest & xtr==1, 1, 0)
TP1
xtr[TP1==1]
xest[TP1==1]
set.seed(1234)
xtr <- sample(1:3, size=100, replace=TRUE)
xest <- sample(1:3, size=100, replace=TRUE)
TP1 <- ifelse(xtr==xest & xtr==1, 1, 0)
TP2 <- ifelse(xtr==xest & xtr==2, 1, 0)
TP3 <- ifelse(xtr==xest & xtr==3, 1, 0)
TP4 <- ifelse(xtr==xest & xtr==4, 1, 0)
TP2
TP1
TP2
TP3
TP4
set.seed(1234)
xtr <- sample(1:3, size=100, replace=TRUE)
xest <- sample(1:3, size=100, replace=TRUE)
TP1 <- ifelse(xtr==xest & xtr==1, 1, 0)
TP2 <- ifelse(xtr==xest & xtr==2, 1, 0)
TP3 <- ifelse(xtr==xest & xtr==3, 1, 0)
TN1 <- ifelse(xest!=1 & xtr!=1, 1, 0)
TN2 <- ifelse(xest!=2 & xtr!=2, 1, 0)
TN3 <- ifelse(xest!=3 & xtr!=3, 1, 0)
TN1
TN2
TN3
FP1 <- ifelse(xest==1 & xtr!=1, 1, 0)
FP2 <- ifelse(xest==2 & xtr!=2, 1, 0)
FP3 <- ifelse(xest==3 & xtr!=3, 1, 0)
FP1
FP2
set.seed(1234)
xtr <- sample(1:3, size=100, replace=TRUE)
xest <- sample(1:3, size=100, replace=TRUE)
TP1 <- ifelse(xtr==xest & xtr==1, 1, 0)
TP2 <- ifelse(xtr==xest & xtr==2, 1, 0)
TP3 <- ifelse(xtr==xest & xtr==3, 1, 0)
TN1 <- ifelse(xest!=1 & xtr!=1, 1, 0)
TN2 <- ifelse(xest!=2 & xtr!=2, 1, 0)
TN3 <- ifelse(xest!=3 & xtr!=3, 1, 0)
FP1 <- ifelse(xest==1 & xtr!=1, 1, 0)
FP2 <- ifelse(xest==2 & xtr!=2, 1, 0)
FP3 <- ifelse(xest==3 & xtr!=3, 1, 0)
FN1 <- ifelse(xest!=1 & xtr==1, 1, 0)
FN2 <- ifelse(xest!=2 & xtr==2, 1, 0)
FN3 <- ifelse(xest!=3 & xtr==3, 1, 0)
(sum(TP1) + sum(TP2) + sum(TP3))/(sum(TP1) + sum(TP2) + sum(TP3) + sum(FP1) + sum(FP2) + sum(FP3))
(sum(TP1)/(sum(TP1) + sum(FP1)) + sum(TP2)/(sum(TP2) + sum(FP2)) + sum(TP3)/(sum(TP3) + sum(FP3)))/3
toupper("Kiritchenko")
toupper("CaiHofmann2007")
toupper("Cesa-Bianchi2006")
(2/3)*1 + (1/3)*1.3
(2/3)*1 + (1/3)*1.7
library("xtable")
# NOT RUN {
## Load example dataset
data(tli)
## Demonstrate data.frame
tli.table <- xtable(tli[1:20, ])
tli.table
- [ ] Please **add helpful comments** to your code:
- [ ] Each user-defined function must be documented including all function arguments and returned objects.
---
09
--9
---9
----9
-----------------------------9
---------------------------------9
------------------------------------9
install.packages("HieRanFor", repos="http://R-Forge.R-project.org")
nchar("if(data.table::between(Betas[1], quantile(matriz_amostras_uniforme[,1],probs = 0.025),quantile(matriz_amostras_uniforme[,1],probs = 0.975))){Cont_Cobertura_uniforme[1]=Cont_Cobertura_uniforme[1]+1}")
114 + 67
85.50 + 66.40 + 89.90
72/3
82103/536
19 + 60 + 25
cor(c(1,3), c(3, 10))
cor(c(1,3), c(3, 14))
cor(c(1,3), c(2, 14))
X <- cbind(rnorm(500), rnorm(500))
plot(X[,1], X[,2]-X[,1])
gen_tree_structure <- function(maxnleaf=20) {
nodelist <- list()
leftnodes <- rightnodes <- 0
count <- 1
leftnodes[count] <- rightnodes[count] <- 1
count <- count+1
numnodes <- sample(2:3, size=1, prob=c(2,1))
numnodes
if(numnodes > maxnleaf)
stop("bla bla bla.")
nodelist[[1]] <- 1 + (1:numnodes)
leftnode <- min(nodelist[[1]])
rightnode <- max(nodelist[[1]])
nleaf <- rightnode-(leftnode-1)
leftnodes[count] <- leftnode
rightnodes[count] <- rightnode
count <- count+1
while(nleaf <= maxnleaf) {
for(i in leftnode:rightnode) {
numnodes <- sample(2:3, size=1, prob=c(2,1))
nodelist[[i]] <- max(nodelist[[i-1]]) + (1:numnodes)
}
leftnode <- min(nodelist[[leftnode]])
rightnode <- max(nodelist[[rightnode]])
nleaf <- rightnode-(leftnode-1)
leftnodes[count] <- leftnode
rightnodes[count] <- rightnode
count <- count+1
}
leftnodes[length(leftnodes)-2]
nodelist <- nodelist[1:rightnodes[length(rightnodes)-2]]
return(nodelist)
}
gen_tree_structure(20)
5/6
library("ordinalForest")
?ordfor
library("ranger")
?ranger
0.05*500
30/500
library(ranger)
?predict.ranger
library(diversityForest)
integer.to.factor
rm(list=ls());gc()
^1^4
2^4
2^4-1
remove.packages()
remove.packages("hierclass")
library("devtools")
install_github("RomanHornung/hierclass")
?topdown
# Load the required packages:
library("mlr3")
library("hierclass")
?topdown
class(X)
setwd("Z:/Projects/DESTATIS/PredErrorComplex/PPerfEstComplex")
source("./simulations/hierpr/functions.R")
if (FALSE) {
# Generate the tree structure:
# Keep generating new tree structures until a tree structure
# with exactly 50 leaf nodes results:
nleaf <- 0
count <- 0
set.seed(12)
while(nleaf!=50) {
tempobj <- gen_tree_structure(50)
nleaf <- tempobj$rightnodes[length(tempobj$rightnodes)] - tempobj$leftnodes[length(tempobj$leftnodes)] + 1
count <- count + 1
}
count
treestruc <- tempobj
plot_structure(treestruc)
save(treestruc, file="./simulations/hierpr/results/intermediate_results/treestruc.Rda")
sum(sapply(1:length(treestruc$leftnodes), function(x) treestruc$rightnodes[x] - treestruc$leftnodes[x] + 1))
}
# -	Erster Schritt ist die hierarchische Baumstruktur festlegen.
# -	Jeder interne Knoten ist mit einem lokalen Modell verknÃ¼pft, wobei die Unterscheidbarkeit der Klassen geringer werden soll, je weiter unten man in der hierarchischen Struktur ankommt. Das wird realisiert in dem das Rauschen (epsilon) immer weiter vergrÃ¶Ãert wird, je weiter man nach unten kommt.
# -	Dass die Koeffizienten von vertikal benachbarten Schichten sich Ã¤hnlich sind, wird so realisiert, dass die Koeffizenten aus Normalverteilungen mit Mittelwerten gleich den Koeffizienten der darÃ¼berliegenden Schichten gezogen werden.
# unterschiedlche intercept sind, wichtig, damit ungeliche klassengrÃ¶Ãen entstehen.
# die intercept von benachbarten schichten sollen nicht voneinander abhÃ¤ngen, weil
# die degrees der klassenungleichheiten nicht voneinander abhÃ¤gne sollen.
# n <- 50000
#
# # erster coefficent intercept, rest sind die beeinflussenden betas:
# vCoef1 = rep(0, 6)
# vCoef2 = rnorm(6)
# vCoef3 = rnorm(6)
#
# # sim_multinom <- function(n, coefs) {
#
# mX = cbind(rep(1, n), matrix(rnorm(n*5), n, 5))
#
# # vector of probabilities
# vProb = cbind(exp(mX%*%vCoef1), exp(mX%*%vCoef2), exp(mX%*%vCoef3))
#
# # multinomial draws
# mChoices = t(apply(vProb, 1, rmultinom, n = 1, size = 1))
# dfM = cbind.data.frame(y = apply(mChoices, 1, function(x) which(x==1)), mX)
#
# library("nnet")
# m <- multinom(y ~ ., data = dfM)
# summary(m)
#
# vCoef2
# vCoef3
50*20
50*50
# 1. coefficenten simulieren:
#
# 2. Daten simulieren
# 3. Daten testen:
# da brauch ich
load("./simulations/hierpr/results/intermediate_results/treestruc.Rda")
# Variance of the normal distribution from which the intercepts are drawn:
# sdbeta0 <- sqrt(1)
# The betas have layer-specific variances:
# sdbeta <- sqrt(c(1, 1.5, 2, 2.5, 3))
# treestruc
simulate_coefs <- function(treestruc, sdbeta0=sqrt(1),
sdbeta=sqrt(c(1, 1.5, 2, 2.5, 3))) {
coeflist <- vector(mode = "list", length = length(treestruc$nodelist))
for(i in seq(along=coeflist)) {
coeflist[[i]] <- list()
# Add the information on the child nodes for each node:
coeflist[[i]]$childnodes <- treestruc$nodelist[[i]]
# Add the information on the parent nodes for each node:
coeflist[[i]]$parentnodes <- which(sapply(1:length(coeflist), function(x) i %in% coeflist[[x]]$childnodes))
# Add the layer of each node:
coeflist[[i]]$layer <- which(sapply(1:length(treestruc$leftnodes), function(x) (i >= treestruc$leftnodes[x]) & (i <= treestruc$rightnodes[x])))
}
# Simulate the coefficients:
maxlayer <- max(sapply(coeflist, function(x) x$layer))
for(i in seq(along=coeflist)) {
# if(coeflist[[i]]$layer==maxlayer) {
#   coeflist[[i]]$coefs <- NA
# } else {
if(length(coeflist[[i]]$childnodes)==2) {
coefs <- matrix(nrow=1, ncol=6, data=c(rnorm(1, sd=sdbeta0), rnorm(5, sd=sdbeta[coeflist[[i]]$layer])))
}
if(length(coeflist[[i]]$childnodes)==3) {
coefs <- rbind(c(rnorm(1, sd=sdbeta0), rnorm(5, sd=sdbeta[coeflist[[i]]$layer])),
c(rnorm(1, sd=sdbeta0), rnorm(5, sd=sdbeta[coeflist[[i]]$layer])))
}
coeflist[[i]]$coefs <- coefs
# }
}
return(coeflist)
}
# Function that takes the covariate matrix of the subset of observations
# contained in a node and the coefficients associated with that node
# to output the indices of the child nodes to which the observations
# get assigned to.
get_child_nodes <- function(Xsub, coefs) {
desmat <- cbind(1, Xsub)
if (nrow(coefs)==2)
vProb <- cbind(1, exp(desmat%*%coefs[1,]), exp(desmat%*%coefs[2,]))
else
vProb <- cbind(1, exp(desmat%*%coefs[1,]))
mChoices <- t(apply(vProb, 1, rmultinom, n = 1, size = 1))
ys <- apply(mChoices, 1, function(x) which(x==1))
return(ys)
}
# Simulate a whole dataset:
n <- 5000
X <- matrix(nrow=n, ncol=5, rnorm(n*5))
# innerclasses
set.seed(1234)
#asdf
coeflist <- simulate_coefs(treestruc=treestruc, sdbeta0=sqrt(1),
sdbeta=sqrt(c(1, 1.5, 2, 2.5, 3)))
# NAECHSTER SCHRITT: SCHAUEN, OB DIE SIMULATIONSPARAMETER PASSEN,
# INDEM GESCHAUT WIRD WIR GUT DIE MODELLE IN DEN EINZELNEN KNOTEN
# SIND UND AUCH WIE UNGLEICH GROS DIE ENDKNOTEN SIND (VIELLEICHT DAS AUCH
# VISUALIIEREN).
# NATÃRLICH AUCH HIERCLASS DRÃBER LAUFEN LASSEN, UM ZU SEHEN, WIE
# GUT DIE PRÃDIKTION FUNKTIONIERT.
plot_structure(treestruc)
maxlayer <- max(sapply(coeflist, function(x) x$layer))
outcomemat <- matrix(nrow=nrow(X), ncol=maxlayer)
tempclass <- coeflist[[1]]$childnodes[get_child_nodes(X, coeflist[[1]]$coefs)]
outcomemat[,1] <- tempclass
coeflist[[1]]$datanode <- data.frame(X)
coeflist[[1]]$datanode$y <- factor(tempclass)
for(i in 2:(length(coeflist)-1)) {
subs <- outcomemat[,coeflist[[i]]$layer-1]==i
tempclass <- coeflist[[i]]$childnodes[get_child_nodes(X[subs,], coeflist[[i]]$coefs)]
outcomemat[subs,coeflist[[i]]$layer] <- tempclass
coeflist[[i]]$datanode <- data.frame(X[subs,])
coeflist[[i]]$datanode$y <- factor(tempclass)
}
# fix(outcomemat)
ystring <- apply(outcomemat, 1, function(x) paste(x, collapse="."))
data <- data.frame(X)
class(X)
head(data)
data$y <- factor(ystring)
# Load the required packages:
library("mlr3")
library("hierclass")
# Set seed for reproducibility:
set.seed(1234)
# Define the task for the top-down classification rule:
task = as_task_classif(y ~ ., data = data)
# Initialize the learner for the top-down classification rule:
learner = lrn("classif.topdown")
dim(data)
# Train a model using the above learner for a subset of the task:
learner$train(task, row_ids = 1:800)
dim(data)
head(data)
# Train a model using the above learner for a subset of the task:
learner$train(task, row_ids = 1:400)
# Train a model using the above learner for a subset of the task:
learner$train(task, row_ids = 1:4000)
learner$model
# Load example dataset:
data(datasim)
# Define the task for the top-down classification rule:
task = as_task_classif(ydepvar ~ ., data = datasim)
# Initialize the learner for the top-down classification rule:
learner = lrn("classif.topdown")
Train and Predict
# Train a model using the above learner for a subset of the task:
learner$train(task, row_ids = 1:300)
learner$model
head(datasim$ydepvar)
head(data$y)
fix(outcomemat)
any(is.na(outcomemat))
grep("NA", ystring, value=TRUE)
table(grep("NA", ystring, value=TRUE))
table(outcomemat[,5])
length(table(outcomemat[,5]))
90 - 40
unique(outcomemat[,5])
setdiff(41:90, unique(outcomemat[,5]))
aha <- coeflist[[40]]
aha
aha <- coeflist[[39]]
aha
# innerclasses
set.seed(1234)
#asdf
coeflist <- simulate_coefs(treestruc=treestruc, sdbeta0=sqrt(1),
sdbeta=sqrt(c(1, 1.5, 2, 2.5, 3)))
# NAECHSTER SCHRITT: SCHAUEN, OB DIE SIMULATIONSPARAMETER PASSEN,
# INDEM GESCHAUT WIRD WIR GUT DIE MODELLE IN DEN EINZELNEN KNOTEN
# SIND UND AUCH WIE UNGLEICH GROS DIE ENDKNOTEN SIND (VIELLEICHT DAS AUCH
# VISUALIIEREN).
# NATÃRLICH AUCH HIERCLASS DRÃBER LAUFEN LASSEN, UM ZU SEHEN, WIE
# GUT DIE PRÃDIKTION FUNKTIONIERT.
plot_structure(treestruc)
maxlayer <- max(sapply(coeflist, function(x) x$layer))
outcomemat <- matrix(nrow=nrow(X), ncol=maxlayer)
tempclass <- coeflist[[1]]$childnodes[get_child_nodes(X, coeflist[[1]]$coefs)]
outcomemat[,1] <- tempclass
coeflist[[1]]$datanode <- data.frame(X)
coeflist[[1]]$datanode$y <- factor(tempclass)
for(i in 2:length(coeflist)) {
subs <- outcomemat[,coeflist[[i]]$layer-1]==i
tempclass <- coeflist[[i]]$childnodes[get_child_nodes(X[subs,], coeflist[[i]]$coefs)]
outcomemat[subs,coeflist[[i]]$layer] <- tempclass
coeflist[[i]]$datanode <- data.frame(X[subs,])
coeflist[[i]]$datanode$y <- factor(tempclass)
}
ystring <- apply(outcomemat, 1, function(x) paste(x, collapse="."))
table(grep("NA", ystring, value=TRUE))
length(unique(ystring))
data <- data.frame(X)
data$y <- factor(ystring)
# Set seed for reproducibility:
set.seed(1234)
# Define the task for the top-down classification rule:
task = as_task_classif(y ~ ., data = data)
# Initialize the learner for the top-down classification rule:
learner = lrn("classif.topdown")
# Train a model using the above learner for a subset of the task:
learner$train(task, row_ids = 1:4000)
traceback()
fix(data)
any(is.an(data))
any(is.na(data))
head(datasim$ydepvar)
head(data$y)
# Train a model using the above learner for a subset of the task:
learner$train(task, row_ids = 1:4000)
?topdown
ah <- topdown(data, dependent.variable.name = "y")
ah <- topdown(data=data[1:2000,], dependent.variable.name = "y")
ah <- topdown(y~., data=data[1:2000,])
traceback()
ah <- topdown(y~., data=data[1:200,])
traceback()
