drawtemp <- sample(which(stat_counts>0), size=2)
epidemiologist_question_pairs[[i]] <- paste0("s_", drawtemp)
stat_counts[drawtemp] <- stat_counts[drawtemp] - 1
}
})
cond1 <- all(!mapply(function(x, y) as.numeric(gsub("e_", "", gsub("s_", "", x))) %in% as.numeric(gsub("e_", "", gsub("s_", "", y))),
names(statistician_question_pairs), statistician_question_pairs))
cond2 <- all(!mapply(function(x, y) as.numeric(gsub("e_", "", gsub("s_", "", x))) %in% as.numeric(gsub("e_", "", gsub("s_", "", y))),
names(epidemiologist_question_pairs), epidemiologist_question_pairs))
count <- count + 1
}
statistician_question_pairs
epidemiologist_question_pairs
# Vector for all statisticians and epidemiologists
n <- 8
statisticians <- paste0("s_", 1:n)
epidemiologists <- paste0("e_", 1:n)
count <- 1
set.seed(12)
while(!(cond1 & cond2)) {
stat_counts <- rep(2, 8)
epid_counts <- rep(2, 8)
# try({
for(i in seq(along=statistician_question_pairs)) {
drawtemp <- sample(which(epid_counts>0), size=2)
statistician_question_pairs[[i]] <- paste0("e_", drawtemp)
epid_counts[drawtemp] <- epid_counts[drawtemp] - 1
drawtemp <- sample(which(stat_counts>0), size=2)
epidemiologist_question_pairs[[i]] <- paste0("s_", drawtemp)
stat_counts[drawtemp] <- stat_counts[drawtemp] - 1
}
# })
cond1 <- all(!mapply(function(x, y) as.numeric(gsub("e_", "", gsub("s_", "", x))) %in% as.numeric(gsub("e_", "", gsub("s_", "", y))),
names(statistician_question_pairs), statistician_question_pairs))
cond2 <- all(!mapply(function(x, y) as.numeric(gsub("e_", "", gsub("s_", "", x))) %in% as.numeric(gsub("e_", "", gsub("s_", "", y))),
names(epidemiologist_question_pairs), epidemiologist_question_pairs))
count <- count + 1
}
statistician_question_pairs
epidemiologist_question_pairs
cond1
cond2
# Vector for all statisticians and epidemiologists
n <- 8
statisticians <- paste0("s_", 1:n)
epidemiologists <- paste0("e_", 1:n)
count <- 1
set.seed(12)
cond1 <- cond2 <- FALSE
while(!(cond1 & cond2)) {
stat_counts <- rep(2, 8)
epid_counts <- rep(2, 8)
try({
for(i in seq(along=statistician_question_pairs)) {
drawtemp <- sample(which(epid_counts>0), size=2)
statistician_question_pairs[[i]] <- paste0("e_", drawtemp)
epid_counts[drawtemp] <- epid_counts[drawtemp] - 1
drawtemp <- sample(which(stat_counts>0), size=2)
epidemiologist_question_pairs[[i]] <- paste0("s_", drawtemp)
stat_counts[drawtemp] <- stat_counts[drawtemp] - 1
}
})
cond1 <- all(!mapply(function(x, y) as.numeric(gsub("e_", "", gsub("s_", "", x))) %in% as.numeric(gsub("e_", "", gsub("s_", "", y))),
names(statistician_question_pairs), statistician_question_pairs))
cond2 <- all(!mapply(function(x, y) as.numeric(gsub("e_", "", gsub("s_", "", x))) %in% as.numeric(gsub("e_", "", gsub("s_", "", y))),
names(epidemiologist_question_pairs), epidemiologist_question_pairs))
count <- count + 1
}
statistician_question_pairs
epidemiologist_question_pairs
count
5.6/(39+(51/60))
60*5.6/(39+(51/60))
48 - 7
# Load the necessary packages
library(mlr3)
library(mlr3viz)
# Create a synthetic dataset
set.seed(123)
data <- data.frame(
x1 = rnorm(105),
x2 = rnorm(105),
y = c(rep("class1", 50), rep("class2", 45), rep("class3", 10))
)
# Create a mlr3 task
task <- TaskClassif$new(id = "mydata", backend = data, target = "y")
# Load the necessary packages
library(mlr3)
library(mlr3viz)
# Create a synthetic dataset
set.seed(123)
data <- data.frame(
x1 = rnorm(105),
x2 = rnorm(105),
y = factor(c(rep("class1", 50), rep("class2", 45), rep("class3", 10))) # convert to factor
)
# Create a mlr3 task
task <- TaskClassif$new(id = "mydata", backend = data, target = "y")
# Create 10-fold cross-validation
cv <- rsmp("cv", folds = 10)
cv$instantiate(task)
# Check which observation is in which fold
folds_assignment <- lapply(seq_len(cv$iters), function(i) {
list(
test_idx = cv$instance(i)$test_set,
test_data = task$data(cv$instance(i)$test_set)
)
})
# Load the necessary packages
library(mlr3)
library(mlr3viz)
# Create a synthetic dataset
set.seed(123)
data <- data.frame(
x1 = rnorm(105),
x2 = rnorm(105),
y = factor(c(rep("class1", 50), rep("class2", 45), rep("class3", 10))) # convert to factor
)
# Create a mlr3 task
task <- TaskClassif$new(id = "mydata", backend = data, target = "y")
# Create 10-fold cross-validation
cv <- rsmp("cv", folds = 10)
cv$instantiate(task)
# Check which observation is in which fold
folds_assignment <- lapply(seq_len(cv$iters), function(i) {
test_idx = task$row_roles$test[task$col_roles$resampling[[i]]]
test_data = task$data()[test_idx, ]
list(
test_idx = test_idx,
test_data = test_data
)
})
# Print the folds assignment
folds_assignment
?rsmp
# Load the necessary packages
library(mlr3)
library(mlr3viz)
# Create a synthetic dataset
set.seed(123)
data <- data.frame(
x1 = rnorm(105),
x2 = rnorm(105),
y = factor(c(rep("class1", 50), rep("class2", 45), rep("class3", 10))) # convert to factor
)
# Create a mlr3 task
task <- TaskClassif$new(id = "mydata", backend = data, target = "y")
# Set the stratification column
task$col_roles$stratum = task$target_names
# Create 10-fold stratified cross-validation
cv <- rsmp("cv", folds = 10)
cv$instantiate(task)
# Check which observation is in which fold
folds_assignment <- lapply(seq_len(cv$iters), function(i) {
test_idx = task$row_roles$test[task$col_roles$resampling[[i]]]
test_data = task$data()[test_idx, ]
list(
test_idx = test_idx,
test_data = test_data
)
})
# Print the folds assignment
folds_assignment
task$row_roles$test
names(cv)
cv$train_set()
cv$train_set
task$row_roles
# Load the necessary packages
library(mlr3)
# Create a synthetic dataset
set.seed(123)
data <- data.frame(
x1 = rnorm(105),
x2 = rnorm(105),
y = factor(c(rep("class1", 50), rep("class2", 45), rep("class3", 10))) # convert to factor
)
# Create a mlr3 task
task <- TaskClassif$new(id = "mydata", backend = data, target = "y")
# Create 10-fold cross-validation
cv <- rsmp("cv", folds = 10)
cv$instantiate(task)
# Check which observation is in which fold
folds_assignment <- lapply(seq_len(cv$iters), function(i) {
list(
test_idx = cv$instance(i)$test_set,
test_data = task$data(cv$instance(i)$test_set)
)
})
task = TaskClassif$new(id = "penguins", backend = palmerpenguins::penguins, target = "species")
resampling = rsmp("cv", folds = 3)
resampling$instantiate(task)
resampling$iters
resampling$iters[[1]]$train_set
resampling$iters
names(resampling)
task = TaskClassif$new(id = "penguins", backend = palmerpenguins::penguins, target = "species")
resampling = rsmp("cv", folds = 3)
resampling$instantiate(task)
resampling$iters
resampling$iters[[1]]$train_set
# Define task
task <- mlr3::TaskClassif$new(id = "penguins", backend = palmerpenguins::penguins, target = "species")
# Define resampling strategy
resampling <- mlr3::rsmp("cv", folds = 3)
# Instantiate the resampling
resampling$instantiate(task)
# Check the training set indices for the first fold
resampling$instance[[1]]$train_set()
# Define task
task <- mlr3::TaskClassif$new(id = "penguins", backend = palmerpenguins::penguins, target = "species")
# Define resampling strategy
resampling <- mlr3::rsmp("cv", folds = 3)
# Instantiate the resampling
resampling$instantiate(task)
# Check the training set indices for the first fold
resampling$instance[[1]]$train_set()
# Define task
task <- mlr3::TaskClassif$new(id = "penguins", backend = palmerpenguins::penguins, target = "species")
# Define resampling strategy
resampling <- mlr3::rsmp("cv", folds = 3)
# Instantiate the resampling
resampling$instantiate(task)
# Check the training set indices for the first fold
resampling$train_set(1)
# Check the test set indices for the first fold
resampling$test_set(1)
# Check the training set indices for the first fold
resampling$train_set(4)
# Check the training set indices for the first fold
resampling$train_set(3)
# Check the test set indices for the first fold
resampling$test_set(1)
# Load the necessary packages
library(mlr3)
library(mlr3viz)
# Create a synthetic dataset
set.seed(123)
data <- data.frame(
x1 = rnorm(105),
x2 = rnorm(105),
y = factor(c(rep("class1", 50), rep("class2", 45), rep("class3", 3))) # convert to factor
)
task = as_task_classif(y ~ ., data = data)
task$col_roles$stratum = task$target_names
# Define resampling strategy
resampling <- mlr3::rsmp("cv", folds = 3)
# Instantiate the resampling
resampling$instantiate(task)
# Check the training set indices for the first fold
resampling$train_set(1)
# Check the training set indices for the first fold
table(datda$y[resampling$train_set(1)])
# Check the training set indices for the first fold
table(data$y[resampling$train_set(1)])
table(dadta$y)
table(data$y)
# Load the necessary packages
library(mlr3)
library(mlr3viz)
# Create a synthetic dataset
set.seed(123)
data <- data.frame(
x1 = rnorm(105),
x2 = rnorm(105),
y = factor(c(rep("class1", 50), rep("class2", 45), rep("class3", 3))) # convert to factor
)
# Load the necessary packages
library(mlr3)
library(mlr3viz)
# Create a synthetic dataset
set.seed(123)
data <- data.frame(
x1 = rnorm(98),
x2 = rnorm(98),
y = factor(c(rep("class1", 50), rep("class2", 45), rep("class3", 3))) # convert to factor
)
task = as_task_classif(y ~ ., data = data)
task$col_roles$stratum = task$target_names
# Define resampling strategy
resampling <- mlr3::rsmp("cv", folds = 3)
# Instantiate the resampling
resampling$instantiate(task)
# Check the training set indices for the first fold
table(data$y[resampling$train_set(1)])
table(data$y[resampling$train_set(2)])
table(data$y[resampling$train_set(3)])
table(data$y[resampling$train_set(4)])
# Load the necessary packages
library(mlr3)
library(mlr3viz)
# Create a synthetic dataset
set.seed(123)
data <- data.frame(
x1 = rnorm(98),
x2 = rnorm(98),
y = factor(c(rep("class1", 50), rep("class2", 45), rep("class3", 3))) # convert to factor
)
task = as_task_classif(y ~ ., data = data)
task$col_roles$stratum = task$target_names
# Define resampling strategy
resampling <- mlr3::rsmp("cv", folds = 10)
# Instantiate the resampling
resampling$instantiate(task)
# Check the training set indices for the first fold
table(data$y[resampling$train_set(1)])
table(data$y[resampling$train_set(2)])
table(data$y[resampling$train_set(3)])
table(data$y[resampling$train_set(4)])
# Check the training set indices for the first fold
table(data$y[resampling$test_set(1)])
table(data$y[resampling$test_set(2)])
table(data$y[resampling$test_set(3)])
table(data$y[resampling$test_set(4)])
table(data$y[resampling$test_set(5)])
data$y[resampling$test_set(1)]
resampling$test_set(1)
# Load the necessary packages
library(mlr3)
library(mlr3viz)
# Create a synthetic dataset
set.seed(123)
data <- data.frame(
x1 = rnorm(98),
x2 = rnorm(98),
y = sample(factor(c(rep("class1", 50), rep("class2", 45), rep("class3", 3)))) # convert to factor
)
task = as_task_classif(y ~ ., data = data)
task$col_roles$stratum = task$target_names
# Define resampling strategy
resampling <- mlr3::rsmp("cv", folds = 10)
# Instantiate the resampling
resampling$instantiate(task)
# Check the training set indices for the first fold
table(data$y[resampling$test_set(1)])
resampling$test_set(1)
plot(resampling$test_set(1))
data$y[resampling$test_set(1)]
resampling$test_set(1)
(13 + 6/60) - (11 + 14/60) + (19 + 28/60) - (13 + 36/60)
14 - (9+20/60) + (15+17/60) - (14+25/60) + (16+25/60) - (16+7/60)
8 - 5.833333
2.166666666666 * 60
library(ordinalForest)
?ordfor
?predict.ordfor
(13 + (35/60)) - 9.5 + (17 + (50/60)) - (14 + (15/60))
.666667-0.5
0.166667 * 60
(13 + 10/60) - (8 + 45/60) + (15 + 5/60) - (13 + 42/60)
0.3*60
21-2.5
ui <- readLines("C:/Projects/MitnehmenBreeze.txt")
ui
ui2 <- sort(ui)
ui2
fileConn<-file("C:/Projects/MitnehmenBreeze_2.txt")
writeLines(u2i, fileConn)
close(fileConn)
fileConn<-file("C:/Projects/MitnehmenBreeze_2.txt")
writeLines(ui2, fileConn)
close(fileConn)
16/1.5
16/2
17/1.5
(12 + 25/60) - (9 + 30/60) + (13 + 51/60) - (12 + 40/60) + (15 + 50/60) - (14 + 56/60) + (19 + 40/60) - (10 + 0/60) + (20 + 30/60) - (20 + 8/60)
(12 + 25/60) - (9 + 30/60) + (13 + 51/60) - (12 + 40/60) + (15 + 50/60) - (14 + 56/60) + (19 + 40/60) - (16 + 0/60) + (20 + 30/60) - (20 + 8/60)
7.49*12*7
(11 + (44/60)) - (9 + 43/60) + (12 + 21/60) - (11 + 58/60) + (17 + 30/60) - (13 + 21/60)
ui <- function(p) 1 - (p^2 + (1-p)^2)
plot(ui, 0, 1)
ui2 <- function(p) (1-p^2)/(p^2)
plot(ui2, 0, 0.3)
plot(ui2, 0, 0.1)
plot(ui2, 0, 0.01)
plot(ui2, 0, 0.3)
(13 + 20/60) - (12 + 5/60) + (22 + 10/60) - (15)
60 * .416667
15/9
20/1.666667
9*20/11
16.36364/9
20/11
ui <- c(194, 544, 781, 223, 356)
plot(sort(ui))
diff(sort(ui))
plot(diff(sort(ui)))
(11 + 39/60) - (9 + 35/60) + (14 + 23/60) - (13 + 28/60) + (15 + 15/60) - (14 + 43/60) + (17 + 31/60) - (15 + 32/60) + (20 + 30/60) - (17 + 40/60)
(19 + 50/60) - (14 + 43/60) + (14 + 9/60) - (9 + 55/60)
library(hierclass)
devtools::install_github("RomanHornung/hierclass")
library(hierclass)
hloss
(14 + 53/60) - (13 + 36/60) + (13 + 13/60) - (10 + 2/60)
(14 + 53/60) - (13 + 36/60) + (13 + 13/60) - (10 + 2/60) + 0.5
library("ordinalForest")
?ordfor
install.packages("scholar")
library("scholar")
?get_citation_history
# id <- "https://scholar.google.de/citations?user=BXflckQAAAAJ&hl"
id <- "BXflckQAAAAJ&hl"
# id <- "https://scholar.google.de/citations?user=BXflckQAAAAJ&hl"
id <- "BXflckQAAAAJ"
# Fetch citation history
citation_history <- get_citation_history(id)
class(citation_history)
dim(citations_history)
dim(citation_history)
citation_history
publications <- get_publications(id)
class(publications)
dim(publications)
publications
fix(publications)
names(publications)
table(publications$journal)
sort(publications$journal, decreasing = TRUE)
sort(table(publications$journal), decreasing = TRUE)
sort(table(tolower(publications$journal)), decreasing = TRUE)
library("scholar")
# Your Google Scholar ID
id <- 'BXflckQAAAAJ'
# Fetch your publications data
publications <- get_publications(id)
# Generate a unique filename with the current date
date_str <- format(Sys.Date(), "%Y%m%d")  # e.g., 20231007
filename <- paste0("publications_", date_str, ".Rda")
filename
?format
# Generate a unique filename with the current date
date_str <- format(Sys.Date(), "%Y_%m_%d")  # e.g., 20231007
filename <- paste0("publications_", date_str, ".Rda")
filename
# Generate a unique filename with the current date
date_str <- format(Sys.Date(), "%d_%m_%Y")  # e.g., 20231007
filename <- paste0("publications_", date_str, ".Rda")
filenma
filenma
filena,e
filename
paste0("C:/Projects/GoogleScholarCitations/", filename)
library("scholar")
# Your Google Scholar ID
id <- 'BXflckQAAAAJ'
# Fetch your publications data
publications <- get_publications(id)
# Generate a unique filename with the current date
date_str <- format(Sys.Date(), "%d_%m_%Y")  # e.g., 20231007
filename <- paste0("publications_", date_str, ".Rda")
# Save the data to an Rda file
save(publications, file = paste0("C:/Projects/GoogleScholarCitations/raw_results/", filename))
(12 + 40/60) - (9 + 10/60) + (16 + 37/60) - (13 + 15/60)
8 - 6.866667
1.133333*60
(12 + (21/60)) - 10 + (16 + 15/60) - (13 + 16/60)
8 - 5.333333
8 - 5.25
tolower("Machine learning, Correlated data, Overoptimism, Resampling, Cross-validation, Spatial data, Clustered data, Unequal sampling probabilities, Concept drift, Hierarchical classification")
15/25
20*15/25
25*5
# Simulate four data sets: 1) no label shift, no feature shift,
# 2) no label shift, feature shift, 3) label shift, no feature shift,
# 4) label shift, feature shift.
source("./concdrift/functions.R")
setwd("C:/Projects/DESTATIS/PredErrorComplex/PPerfEstComplex/PPerfEstComplex")
# Simulate four data sets: 1) no label shift, no feature shift,
# 2) no label shift, feature shift, 3) label shift, no feature shift,
# 4) label shift, feature shift.
source("./concdrift/functions.R")
n <- 500
seasonbreaks <- seq(0, 1, length=11)
seasonbreaks <- c(seasonbreaks[1:9], (seasonbreaks[9]+seasonbreaks[10])/2,
seasonbreaks[10], (seasonbreaks[10]+seasonbreaks[11])/2,
seasonbreaks[11])
set.seed(1234)
x1muend <- 0; x2muend <- 0; x3muend <- 0
ymuend <- 0; yvarend <- 1
datatrain_Noyshift_Nocovshift <- sim_dataset(seq(seasonbreaks[1], seasonbreaks[9], length=n),
x1muend=x1muend, x2muend=x2muend, x3muend=x3muend,
ymuend=ymuend, yvarend=yvarend)
x1muend <- 8; x2muend <- 4; x3muend <- -8
ymuend <- 0; yvarend <- 1
datatrain_Noyshift_covshift <- sim_dataset(seq(seasonbreaks[1], seasonbreaks[9], length=n),
x1muend=x1muend, x2muend=x2muend, x3muend=x3muend,
ymuend=ymuend, yvarend=yvarend)
x1muend <- 0; x2muend <- 0; x3muend <- 0
ymuend <- 8; yvarend <- 8
datatrain_yshift_Nocovshift <- sim_dataset(seq(seasonbreaks[1], seasonbreaks[9], length=n),
x1muend=x1muend, x2muend=x2muend, x3muend=x3muend,
ymuend=ymuend, yvarend=yvarend)
x1muend <- 8; x2muend <- 4; x3muend <- -8
ymuend <- 8; yvarend <- 8
datatrain_yshift_covshift <- sim_dataset(seq(seasonbreaks[1], seasonbreaks[9], length=n),
x1muend=x1muend, x2muend=x2muend, x3muend=x3muend,
ymuend=ymuend, yvarend=yvarend)
# Compare predictions obtained with the different data sets:
# Predictions into the future:
library("ranger")
mod_Noyshift_Nocovshift <- ranger(dependent.variable.name = "y", data=datatrain_Noyshift_Nocovshift[1:400,], num.trees=1000)
library("measures")
MSE(truth=datatrain_Noyshift_Nocovshift[-(1:400),]$y, response=predict(mod_Noyshift_Nocovshift, data=datatrain_Noyshift_Nocovshift[-(1:400),])$predictions)
mod_Noyshift_covshift <- ranger(dependent.variable.name = "y", data=datatrain_Noyshift_covshift[1:400,], num.trees=1000)
library("measures")
MSE(truth=datatrain_Noyshift_covshift[-(1:400),]$y, response=predict(mod_Noyshift_covshift, data=datatrain_Noyshift_covshift[-(1:400),])$predictions)
mod_yshift_Nocovshift <- ranger(dependent.variable.name = "y", data=datatrain_yshift_Nocovshift[1:400,], num.trees=1000)
library("measures")
MSE(truth=datatrain_yshift_Nocovshift[-(1:400),]$y, response=predict(mod_yshift_Nocovshift, data=datatrain_yshift_Nocovshift[-(1:400),])$predictions)
mod_yshift_covshift <- ranger(dependent.variable.name = "y", data=datatrain_yshift_covshift[1:400,], num.trees=1000)
library("measures")
MSE(truth=datatrain_yshift_covshift[-(1:400),]$y, response=predict(mod_yshift_covshift, data=datatrain_yshift_covshift[-(1:400),])$predictions)
